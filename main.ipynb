{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFilter\n",
    "from scipy.ndimage import gaussian_filter, map_coordinates \n",
    "from typing import Any\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pages.csv')\n",
    "df_1 = pd.DataFrame(df).rename(columns={'Unnamed: 1': 'fonts'})\n",
    "df_2 = df_1['pages'].str.replace('^img\\\\\\\\', '', regex=True)\n",
    "df_rete = df_1[['pages', 'fonts']].copy()\n",
    "df_rete['pages'] = df_rete['pages'].str.replace('^img\\\\\\\\', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "# Define the folder path where the images are stored\n",
    "image_folder = f\"{current_path}\\\\TheLibrarianFromAlexandria\\\\img\"\n",
    "output_folder = f\"{current_path}\\\\TheLibrarianFromAlexandria\\\\immagini_formattate\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "output_folder_2 = f\"{current_path}\\\\TheLibrarianFromAlexandria\\\\immagini_reteneurale\"\n",
    "if not os.path.exists(output_folder_2):\n",
    "    os.makedirs(output_folder_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione di pre-processing e salvataggio dell'immagine finale (denoised)\n",
    "def preprocess_and_save_final_image(img: Any, img_name: str):\n",
    "    # **1. Conversione in scala di grigi**\n",
    "    gray = img.convert(\"L\")  # 'L' = grayscale\n",
    "    # **2. Binarizzazione**\n",
    "    threshold = 128  # Soglia per la binarizzazione\n",
    "    binary = gray.point(lambda p: 255 if p > threshold else 0)\n",
    "    # **3. Rimozione del Rumore (Denoising)**\n",
    "    denoised = binary.filter(ImageFilter.MedianFilter(size=3))  # Rimozione del rumore tramite filtro mediano\n",
    "    # Salva solo l'immagine denoised\n",
    "    denoised_image_path = os.path.join(output_folder, f\"{img_name}\")\n",
    "    denoised.save(denoised_image_path)  # Salva l'immagine denoised\n",
    "    print(f\"Immagine denoised salvata come: {denoised_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione di Data Augmentation\n",
    "def data_augmentation(image):\n",
    "    # **1. Trasformazioni base con torchvision**\n",
    "    augmentation = transforms.Compose([\n",
    "                      # Rotazione casuale tra -20 e 20 gradi\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Traslazione del 10%\n",
    "        transforms.RandomHorizontalFlip(p=0.5),              # Ribaltamento orizzontale\n",
    "        transforms.RandomVerticalFlip(p=0.3),                # Ribaltamento verticale\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3), # Variazione luminositÃ  e contrasto\n",
    "    ])\n",
    "    augmented_image = augmentation(image)  # Applica le trasformazioni base\n",
    "    return augmented_image  # Restituisce l'immagine trasformata\n",
    "\n",
    "# Funzione per aggiungere rumore\n",
    "def add_noise(image):\n",
    "    arr = np.array(image).astype(np.float32)\n",
    "    noise = np.random.normal(0, 25, arr.shape)\n",
    "    noisy = np.clip(arr + noise, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(noisy)\n",
    "\n",
    "# Funzione per distorsione elastica\n",
    "def elastic_transform(image, alpha=1000, sigma=40):\n",
    "    arr = np.array(image)\n",
    "    shape = arr.shape\n",
    "    dx = gaussian_filter((np.random.rand(*shape)*2 - 1), sigma) * alpha\n",
    "    dy = gaussian_filter((np.random.rand(*shape)*2 - 1), sigma) * alpha\n",
    "    x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n",
    "    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1))\n",
    "    distorted = map_coordinates(arr, indices, order=1, mode='reflect').reshape(shape)\n",
    "    return Image.fromarray(distorted.astype(np.uint8))\n",
    "\n",
    "# Funzione che applica tutte le trasformazioni in sequenza\n",
    "def apply_all_transformations(image, img_name):\n",
    "    # 1. Applica Data Augmentation\n",
    "    augmented_image = data_augmentation(image)\n",
    "    # 2. Aggiungi rumore\n",
    "    noisy_image = add_noise(augmented_image)\n",
    "    # 3. Applica distorsione elastica\n",
    "    final_image = elastic_transform(noisy_image)\n",
    "    # Salva l'immagine finale\n",
    "    final_image_path = os.path.join(output_folder_2, f\"{img_name}\")\n",
    "    final_image.save(final_image_path)  # Salva l'immagine finale\n",
    "    print(f\"Immagine finale salvata come: {final_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itera sulla colonna 'pages' del DataFrame\n",
    "for i in range(len(df)):  # Limita la visualizzazione a 5 immagini\n",
    "    try:\n",
    "        img_name = df_2.iloc[i] # Ottieni il nome del file dal DataFrame\n",
    "        img_path = os.path.join(image_folder, img_name)\n",
    "        # Verifica se il file esiste\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Immagine {img_name} non trovata.\")\n",
    "            continue\n",
    "        \n",
    "        # Carica l'immagine\n",
    "        img = Image.open(img_path)\n",
    "        # Esegui il Data Augmentation e salva le immagini\n",
    "        print(f\"\\nAnalisi immagine {i + 1}: {img_name}\")\n",
    "        preprocess_and_save_final_image(img, img_name)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Errore nell'aprire l'immagine {img_name}: {e}\")\n",
    "\n",
    "# Itera sulla colonna 'pages' del DataFrame\n",
    "for i in range(len(df)):  # Limita la visualizzazione a 5 immagini\n",
    "    try:\n",
    "        img_name_2 = df_2.iloc[i]  # Ottieni il nome del file dal DataFrame\n",
    "        img_path_2 = os.path.join(output_folder, img_name_2)\n",
    "        # Verifica se il file esiste\n",
    "        if not os.path.exists(img_path_2):\n",
    "            print(f\"Immagine {img_name_2} non trovata.\")\n",
    "            continue\n",
    "        # Carica l'immagine\n",
    "        img_2 = Image.open(img_path_2)\n",
    "        # Esegui il pre-processing e salva solo l'ultima versione (denoised)\n",
    "        print(f\"\\nAnalisi immagine {i + 1}: {img_name_2}\")\n",
    "        apply_all_transformations(img_2, img_name_2)\n",
    "    except Exception as e:\n",
    "        print(f\"Errore nell'aprire l'immagine {img_name_2}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convutional Neural Network (ResNet40--> Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 1004 immagini\n",
      "Test set: 252 immagini\n"
     ]
    }
   ],
   "source": [
    "image_folder_rete = f'{current_path}\\\\TheLibrarianFromAlexandria\\\\immagini_reteneurale'\n",
    "image_names = os.listdir(image_folder_rete)  \n",
    "fonts = list(df_rete['fonts'].unique())\n",
    "train_df, test_df = train_test_split(df_rete, test_size=0.2, random_state=42)\n",
    "print(f\"Training set: {len(train_df)} immagini\")\n",
    "print(f\"Test set: {len(test_df)} immagini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Definisci le trasformazioni per il preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),            # Ridimensiona le immagini a 256x256\n",
    "    transforms.CenterCrop(224),        # Ritaglia il centro dell'immagine a 224x224\n",
    "    transforms.ToTensor(),             # Converte l'immagine in un tensor PyTorch\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalizza con la media e la deviazione standard di ImageNet\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder_rete, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_folder_rete = image_folder_rete\n",
    "        self.transform = transform\n",
    "\n",
    "                # Mappa dei font (assicurati che corrisponda ai font nel tuo dataset)\n",
    "        self.font_map = {\"vesta\": 0, \"aureus\": 1, \"roman\": 2, \"cicero\": 3, \"colosseum\": 4,\n",
    "                         \"augustus\": 5, \"consul\": 6, \"laurel\": 7, \"forum\": 8, \"trajan\": 9, \"senatus\": 10}\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_folder_rete, self.dataframe.iloc[idx, 0])  # Ottieni il nome del file dell'immagine\n",
    "        image = Image.open(img_name).convert('RGB') \n",
    "        label = self.dataframe.iloc[idx, 1]  # La colonna 'labels' contiene le etichette\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # Converte il label da stringa a numero usando la mappa\n",
    "        label = self.font_map[label]\n",
    "        # Restituisci i dati come una tupla di tensori (pagine, etichette)\n",
    "        # La variabile `label` dovrebbe essere un singolo tensore, non una tupla\n",
    "        return image, torch.tensor(label)  # Assicurati che il label sia un tensore PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Crea il DataLoader per Training e Test\n",
    "train_dataset = CustomDataset(train_df, image_folder_rete, transform)\n",
    "test_dataset = CustomDataset(test_df, image_folder_rete, transform)\n",
    "batch_size=35\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Riduci il batch_size per tenere conto del numero ridotto di immagini\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Step 6: Carica il modello pre-addestrato (ResNet50)\n",
    "model = models.resnet50(pretrained=True)\n",
    "# Congela i parametri del modello pre-addestrato\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Aggiungi un nuovo classificatore (adatta al numero di classi del tuo dataset)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(df_rete['fonts'].unique()))  # Cambia in base al numero di classi\n",
    "# Sposta il modello sulla GPU se disponibile\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "# Step 7: Definisci la funzione di ottimizzazione e loss\n",
    "criterion = nn.CrossEntropyLoss()  # Funzione di perdita per la classificazione multi-classe\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=0.08, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Variabili per memorizzare i risultati durante l'addestramento\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Aggiungi l'ottimizzatore (SGD con momentum)\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Aggiungi un Learning Rate Scheduler (StepLR)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # Ogni 5 epoche, riduci il learning rate di un fattore 0.1\n",
    "\n",
    "# Early stopping variabili\n",
    "best_accuracy = 0.0\n",
    "patience = 5  # Numero di epoche senza miglioramento prima di fermare l'addestramento\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Step 8: Addestra il modello (per 15 epoche)\n",
    "for epoch in range(15):  # Esegui per 15 epoche\n",
    "    model.train()  # Imposta il modello in modalitÃ  training\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Ciclo di addestramento\n",
    "    for pages, fonts in train_loader:\n",
    "        pages, fonts = pages.to(device), fonts.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pages)\n",
    "        loss = criterion(outputs, fonts)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += fonts.size(0)\n",
    "        correct += (predicted == fonts).sum().item()\n",
    "\n",
    "    # Calcolo della loss e accuracy per il training set\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct / total * 100\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    print(f'Epoch {epoch+1}, Training Loss: {train_loss}, Training Accuracy: {train_accuracy}%')\n",
    "\n",
    "    # Step 9: Testa il modello sul test set\n",
    "    model.eval()  # Imposta il modello in modalitÃ  valutazione\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disabilita il calcolo dei gradienti per il test\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct / total * 100\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    print(f'Test Accuracy: {test_accuracy}%')\n",
    "\n",
    "    # Early Stopping: verifica se la performance Ã¨ migliorata\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    # Fermiamo l'addestramento se non ci sono miglioramenti per 'patience' epoche\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    # Aggiorna il learning rate con il scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "# Salva i risultati in un file Excel o CSV per analisi future (facoltativo)\n",
    "import pandas as pd\n",
    "\n",
    "results = {\n",
    "    'Epoch': list(range(1, len(train_losses) + 1)),\n",
    "    'Training Loss': train_losses,\n",
    "    'Training Accuracy': train_accuracies,\n",
    "    'Test Accuracy': test_accuracies\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Salva in un file CSV o Excel\n",
    "df_results.to_excel('model_results.xlsx', index=False)\n",
    "print(\"Risultati salvati in 'model_results.xlsx'\")\n",
    "\n",
    "# Visualizza i grafici della Loss e Accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Grafico della Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.title('Training Loss per Epoca')\n",
    "plt.xlabel('Epoche')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Grafico della Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.title('Accuracy per Epoca')\n",
    "plt.xlabel('Epoche')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\corra\\OneDrive\\Desktop\\tests\\23-mlproj\\venv\\Lib\\site-packages\\PIL\\Image.py:3442: DecompressionBombWarning: Image size (103857120 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\corra\\OneDrive\\Desktop\\tests\\23-mlproj\\venv\\Lib\\site-packages\\PIL\\Image.py:3442: DecompressionBombWarning: Image size (103803320 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\corra\\OneDrive\\Desktop\\tests\\23-mlproj\\venv\\Lib\\site-packages\\PIL\\Image.py:3442: DecompressionBombWarning: Image size (103803147 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\corra\\OneDrive\\Desktop\\tests\\23-mlproj\\venv\\Lib\\site-packages\\PIL\\Image.py:3442: DecompressionBombWarning: Image size (103811050 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\corra\\OneDrive\\Desktop\\tests\\23-mlproj\\venv\\Lib\\site-packages\\PIL\\Image.py:3442: DecompressionBombWarning: Image size (96828297 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\corra\\OneDrive\\Desktop\\tests\\23-mlproj\\venv\\Lib\\site-packages\\PIL\\Image.py:3442: DecompressionBombWarning: Image size (96819327 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\corra\\OneDrive\\Desktop\\tests\\23-mlproj\\venv\\Lib\\site-packages\\PIL\\Image.py:3442: DecompressionBombWarning: Image size (103761340 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 2.303737681487511, Training Accuracy: 16.83266932270916%\n",
      "Test Accuracy: 23.41269841269841%\n",
      "Epoch 2, Training Loss: 1.8791441218606357, Training Accuracy: 39.243027888446214%\n",
      "Test Accuracy: 39.682539682539684%\n",
      "Epoch 3, Training Loss: 1.5868562048879162, Training Accuracy: 55.079681274900395%\n",
      "Test Accuracy: 46.82539682539682%\n",
      "Epoch 4, Training Loss: 1.449908815581223, Training Accuracy: 56.37450199203188%\n",
      "Test Accuracy: 47.61904761904761%\n",
      "Epoch 5, Training Loss: 1.3155281749264947, Training Accuracy: 63.645418326693225%\n",
      "Test Accuracy: 48.01587301587302%\n",
      "Epoch 6, Training Loss: 1.2616627935705513, Training Accuracy: 61.952191235059765%\n",
      "Test Accuracy: 49.60317460317461%\n",
      "Epoch 7, Training Loss: 1.1825131403988804, Training Accuracy: 66.23505976095618%\n",
      "Test Accuracy: 54.36507936507936%\n",
      "Epoch 8, Training Loss: 1.1449312094984383, Training Accuracy: 66.03585657370517%\n",
      "Test Accuracy: 55.55555555555556%\n",
      "Epoch 9, Training Loss: 1.085456367196708, Training Accuracy: 68.82470119521913%\n",
      "Test Accuracy: 55.158730158730165%\n",
      "Epoch 10, Training Loss: 1.0371320679269989, Training Accuracy: 69.62151394422311%\n",
      "Test Accuracy: 54.36507936507936%\n",
      "Epoch 11, Training Loss: 0.9966046090783744, Training Accuracy: 71.91235059760956%\n",
      "Test Accuracy: 55.55555555555556%\n",
      "Epoch 12, Training Loss: 0.9860867159119968, Training Accuracy: 72.41035856573706%\n",
      "Test Accuracy: 54.36507936507936%\n",
      "Epoch 13, Training Loss: 0.9375025835530512, Training Accuracy: 72.01195219123507%\n",
      "Test Accuracy: 57.14285714285714%\n",
      "Epoch 14, Training Loss: 0.8974105407451761, Training Accuracy: 74.60159362549801%\n",
      "Test Accuracy: 56.34920634920635%\n",
      "Epoch 15, Training Loss: 0.9122385711505495, Training Accuracy: 73.10756972111554%\n",
      "Test Accuracy: 56.74603174603175%\n"
     ]
    }
   ],
   "source": [
    "# Variabili per memorizzare i risultati durante l'addestramento\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Step 8: Addestra il modello (solo per 1 epoca dato il numero ridotto di immagini)\n",
    "for epoch in range(15):  # Esegui piÃ¹ epoche se necessario\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for pages, fonts in train_loader:\n",
    "        pages, fonts = pages.to(device), fonts.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pages)\n",
    "        loss = criterion(outputs, fonts)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += fonts.size(0)\n",
    "        correct += (predicted == fonts).sum().item()\n",
    "\n",
    "    # Calcolo della loss e accuracy per il training set\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct / total * 100\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    print(f'Epoch {epoch+1}, Training Loss: {train_loss}, Training Accuracy: {train_accuracy}%')\n",
    "\n",
    "    # Step 9: Testa il modello sul test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct / total * 100\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    print(f'Test Accuracy: {test_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisci il percorso per la cartella 'model_charts'\n",
    "model_result_path = 'C:\\\\Users\\\\corra\\\\OneDrive\\\\desktop\\\\tests\\\\23-mlproj\\\\model_results'\n",
    "# Crea la cartella se non esiste\n",
    "if not os.path.exists(model_result_path):\n",
    "    os.makedirs(model_result_path)\n",
    "# Crea i grafici\n",
    "plt.figure(figsize=(10, 5))\n",
    "# Grafico della perdita (loss)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.title('Training Loss per Epoca')\n",
    "plt.xlabel('Epoche')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "# Grafico della precisione (accuracy)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.title('Accuracy per Epoca')\n",
    "plt.xlabel('Epoche')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "# Salva i grafici nella cartella 'model_charts'\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(model_result_path, f'training_and_accuracy_charts_{epoch+1}_{batch_size}.png'))\n",
    "# Mostra il grafico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un DataFrame con i risultati\n",
    "results = {\n",
    "    'Epoch': list(range(1, len(train_losses) + 1)),\n",
    "    'Training Loss': train_losses,\n",
    "    'Training Accuracy': train_accuracies,\n",
    "    'Test Accuracy': test_accuracies\n",
    "}\n",
    "df_results = pd.DataFrame(results)\n",
    "# Salva i risultati in un file CSV\n",
    "df_results.to_excel(f'model_results\\\\training_results_{epoch+1}_{batch_size}.xlsx', index=False)\n",
    "print(\"Risultati esportati nel file 'training_results.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
